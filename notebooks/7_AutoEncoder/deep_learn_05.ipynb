{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Table-of-Contents\" data-toc-modified-id=\"Table-of-Contents-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Table of Contents</a></span></li><li><span><a href=\"#Recurrent-Neural-Networks\" data-toc-modified-id=\"Recurrent-Neural-Networks-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Recurrent Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Long-Short-Term-Memory\" data-toc-modified-id=\"Long-Short-Term-Memory-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Long Short Term Memory</a></span></li><li><span><a href=\"#LSTM-in-Keras\" data-toc-modified-id=\"LSTM-in-Keras-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>LSTM in Keras</a></span></li><li><span><a href=\"#Textaufbereitung-für-ein-RNN\" data-toc-modified-id=\"Textaufbereitung-für-ein-RNN-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Textaufbereitung für ein RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras-embedding-layer:\" data-toc-modified-id=\"Keras-embedding-layer:-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Keras embedding layer:</a></span></li></ul></li><li><span><a href=\"#Beispiel:-Newsgroups-from-scratch\" data-toc-modified-id=\"Beispiel:-Newsgroups-from-scratch-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Beispiel: Newsgroups from scratch</a></span></li><li><span><a href=\"#Hausaufgaben\" data-toc-modified-id=\"Hausaufgaben-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Hausaufgaben</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Convolutional Neural Networks (CNN) sind sehr gut darin, statische Muster in Daten zu erkennen. Sie berücksichtigen allerdings keine Strukturen in der Zeit, Strukturen, wie sie typisch sind für die natürliche Sprache oder Videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grundidee der Recurrent NN: Es gibt eine Schleife über die Inputdaten, die jeweils einen Zeitschritt repräsentieren, z.B. jeweils ein Wort in einem Satz oder ein Bild in einem Video. Zugleich gibt es eine Zustand (state) der erinnert und durch den Input aktualisiert und modifiziert wird. \n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\" height=120 width=120 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das obenstehende Bild ist eine Zusammenfassung der Schleife, die ein RNN ausmacht. Wir können uns das auch etwas genauer anschauen, nämlich ausgerollt:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=440 />\n",
    "\n",
    "Die Eingabe x<sub>t</sub> kann z.B. eine Folge von Worten sein. Dann ist x<sub>0</sub> das erste Wort, x<sub>1</sub> das zweite Wort bis hin zum letzten Wort x<sub>t</sub>. Die waagrechten Pfeile repräsentieren den Informationsfluss der über den Zustand geschieht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chollet beschreibt dies ganz gelungen mit diesem Pseudocode (Chollet 2017: 196):\n",
    "\n",
    "    state_t = 0                          #the state at t\n",
    "    for input_t in input_sequence:       #Iterates over sequence elements   \n",
    "        output_t = f(input_t, state_t)   #f is a function, input is input_t and state_t \n",
    "        state_t = output_t               #The previous output becomes the state for the next iteration. \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion f ist die typische Matrixmultiplikation auf die eine Aktivierungsfunktion gesetzt wird. Man kann das also, frei nach Chollet (2017: 197) als Pseudocode so schreiben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    def recurrent_layer(input_sequence, W, U, b)\n",
    "        state_t = [0,...,0]        #len(state_t) == len(output_t)\n",
    "        successive_outputs = []    #output states at timestep t=0 to t=t\n",
    "        for input_t in input_sequence:\n",
    "            output_t = activation(dot(W, input_t) + dot(U, state_t) + b )\n",
    "            successive_outputs.append(output_t)   \n",
    "            state_t = output_t\n",
    "        return successive_outputs\n",
    "    \n",
    "activation(): Aktivierungsfunktion, z.B. sigmoid, tanh usw.<br/>\n",
    "dot(): Matrixmultiplikation (wie beim Dense-Layer)<br/>\n",
    "W: Matrix mit Gewichten für den Input<br/>\n",
    "U: Matrix mit Gewichten für den Zustand<br/>\n",
    "b: bias \n",
    "successive_outputs = Liste, die am Ende alle Ausgaben für alle Eingaben zu allen Zeitschritten enthält"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können nun also noch einmal einen Blick auf die obenstehende Grafik werfen und zoomen diesmal sozusagen in den Kasten A und die waagrechten Pfeile hinein: \n",
    "<img src=\"rnn_1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein grundlegendes Problem eines solche einfachen RNNs besteht darin, dass es zwar Information über die Zeit erhalten kann, aber leider nicht sehr lange. Offensichtlich wird bei jedem Zeitschritt eine Matrixmultiplikation vorgenommen. Das kann zwei Effekte haben. Erstens gehen kleine Werte, die häufig mit kleinen Werten multipliziert werden, sehr schnell gegen 0. Wenn wir etwa Sätze mit 100 Worten untersuchen wollen, dann werden wir, selbst wenn wir immer nur 0.9 * 0.9 multiplizieren, am Ende einen sehr kleinen Wert haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6561398887587544e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9**100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diesen Effekt nennt man den *vanishing gradient*. Die Kehrseite dieses Effekts ist der *exploding gradient*. Selbst wenn man relativ kleine Zahlen sehr häufig miteinander multipliziert, dann werden sie schnell sehr groß:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1088993727807807e+23"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.7**100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beides führt dazu, dass einfache RNNs für die Praxis nicht geeignet sind. Vielmehr verwendet man zumeist eine Variante, die ausdrücklich dafür geschaffen wurde, das vanishing gradient-Problem zu lösen: die *Long Short Term Memory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory\n",
    "LSTMs haben sich in der Praxis etabliert, weil sie das Problem der *vanishing* bzw. *exploding gradients* im Griff haben. Ausgerollt kann man sie sich so vorstellen ![Chollet Bild 6.15](chollet_6-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Berechnung des neuen *carry*-Werts geschieht über drei weitere Matrixmultiplikationen. Wiederum in Chollets Pseudocode:\n",
    "\n",
    "    output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\n",
    "    i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi) \n",
    "    f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf) \n",
    "    k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)\n",
    "    c_t+1 = i_t * k_t + c_t * f_t\n",
    "\n",
    "In jedem Zeitschritt bzw. bei jedem neuen Wort eines Satzes werden also vier Matrixmultiplikationen vorgenommen! In der Literatur werden diese zusätzlichen Operationen zumeist als *Gate* bezeichnet. Man spricht dann von *input gate*, *forget gate*, und *output gate*. \n",
    "\n",
    "![Darstellung LSTM](lstm1.png)\n",
    "![Legende](lstm_legende.png)\n",
    "(Bildquelle: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM in Keras \n",
    "\n",
    "Wie üblich ist die tatsächliche Verwendung von LSTMs in Keras sehr einfach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(LSTM(64))   #64 = Anzahl der units\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textaufbereitung für ein RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der erste Schritt für die Arbeit mit RNNs ist die Aufbereitung der Textdaten. Dieser Schritt besteht wiederum aus zwei Teilschritten. Im ersten segmentieren wir den Text in gleich große Texteinheiten, dann ersetzen wir, wie üblich, die symbolische Repräsentation, also die Worte, durch Zahlen. \n",
    "\n",
    "1) Textsegmentierung\n",
    "Wie wir bei diesem Schritt vorgehen, hängt sehr von unserer Fragestellung und Korpus ab. Wenn wir etwa Tweets klassifizieren wollen, dann können wir einfach 140 bzw. 280 Zeichen lange Einheiten definieren. Was der aktuelle Text davon nicht verwendet, füllen wir dann mit einem padding-Zeichen auf. Wenn wir längere Texte haben, können wir diese entweder in kleinere Blöcke zerlegen und dann die Klassifikation für jeden Block durchführen (und am Ende ein Voting über alle Blöcke eines Textes machen) oder wir samplen einen Textabschnitt aus dem Text bzw. nehmen einfach immer einen gleich langen Abschnitt am Anfang. Wie immer wir vorgehen, am Ende dieses Schritts haben wir einen lauter gleich lange Textsegmente, so dass unsere Trainingsdaten dann aus einer Liste dieser Segmente bestehen.\n",
    "\n",
    "2) Wort->Zahl\n",
    "\n",
    "Bei diesem Schritt ersetzen wir die Worte des Textes durch einen Vektor. Es gibt im wesentlichen drei Ansätze. Wir werden dabei übrigens immer von Tokens sprechen. Tokens sind beliebige gleichförmige sprachliche Einheiten, also z.B. Buchstaben, Worte, Buchstaben-Ngramme oder Wort-Ngramme. Wir denken aber zuerst einmal nur an Worte. \n",
    "* One-Hot-Encoding der Tokens. Nachteil: Vektor für jedes Token ist so groß wie der Wortschatz und die Encodings haben keine brauchbare Bedeutung, so dass sinnverwandte Worte genausoweit von einander entfernt sind wie sinnferne.\n",
    "* Word Embeddings, die direkt auf den Trainingsdaten berechnet werden. Vorteil: Sehr einfach, Nachteil: das Trainingskorpus muss relativ groß sein, um gute Ergebnisse zu erzielen. Außerdem sind moderne Embeddings deutlich raffinierter als das Keras-Embedding. Vortrainierte Embeddings können zudem umfassend Sprachinformationen einbringen, die die Verarbeitungsergebnisse sehr verbessern.\n",
    "* Vortrainierte Embeddings, die auf sehr großen Trainingskorpora berechnet wurden. Hier gibt es verschiedene Techniken: \n",
    "\n",
    "|.|word2vec | glove |fasttext |elmo  | bert|\n",
    "|---|---|---|---|---|---|\n",
    "|Entstehungsjahr |2013  |2014|2016|2018|2018|\n",
    "|deutsche Modelle |x  |x  |x  |  x|x|\n",
    "|oov*|o  |o  |x  |x |x |\n",
    "|kontextsensitiv|o  |o  |o  |x|x|\n",
    "\n",
    "*oov: out of vocabulary\n",
    "\n",
    "Für viele Zwecke sind die FastText-Modelle ein guter Ausgangspunkt, da sie für viele Sprachen vorliegen, gut mit oov-Wörtern umgehen und sich leicht weitertrainieren lassen.  \n",
    "\n",
    "\n",
    "One-Hot-Encoding schauen wir uns nicht an, da unpraktisch. Beschreibung bei Chollet. \n",
    "\n",
    "### Keras embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding \n",
    "\n",
    "vocab_size = 10000 #Wieviele Wörter (der häufigsten Wörter) werden überhaupt verwendet\n",
    "embed_size = 96    #Wie groß ist der Vektor für jedes Wort, je nach Korpus \n",
    "                   #zwischen 64 und 1024\n",
    "embedding_layer = Embedding(vocab_size, embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel: Newsgroups from scratch\n",
    "Quelle: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "Code: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Embedding, Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TEXT_DATA_DIR = \"20_newsgroup\"\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\envs\\keras\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS=20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"glove_dir\"\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100  # dimension der glove vektoren, die wir verwenden\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/20\n",
      "15998/15998 [==============================] - 4s 248us/step - loss: 2.3800 - acc: 0.2178 - val_loss: 1.9144 - val_acc: 0.3298\n",
      "Epoch 2/20\n",
      "15998/15998 [==============================] - 3s 194us/step - loss: 1.5427 - acc: 0.4624 - val_loss: 1.3459 - val_acc: 0.5069\n",
      "Epoch 3/20\n",
      "15998/15998 [==============================] - 3s 194us/step - loss: 1.1901 - acc: 0.5901 - val_loss: 1.1320 - val_acc: 0.6159\n",
      "Epoch 4/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.9653 - acc: 0.6709 - val_loss: 0.9897 - val_acc: 0.6744\n",
      "Epoch 5/20\n",
      "15998/15998 [==============================] - 3s 194us/step - loss: 0.8152 - acc: 0.7223 - val_loss: 0.9207 - val_acc: 0.6947\n",
      "Epoch 6/20\n",
      "15998/15998 [==============================] - 3s 194us/step - loss: 0.7098 - acc: 0.7619 - val_loss: 0.8645 - val_acc: 0.7137\n",
      "Epoch 7/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.5902 - acc: 0.8014 - val_loss: 1.0458 - val_acc: 0.6819\n",
      "Epoch 8/20\n",
      "15998/15998 [==============================] - 3s 196us/step - loss: 0.4927 - acc: 0.8330 - val_loss: 0.9624 - val_acc: 0.6984\n",
      "Epoch 9/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.4259 - acc: 0.8565 - val_loss: 0.8856 - val_acc: 0.7369\n",
      "Epoch 10/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.3675 - acc: 0.8770 - val_loss: 1.1801 - val_acc: 0.6917\n",
      "Epoch 11/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.3067 - acc: 0.8938 - val_loss: 0.9239 - val_acc: 0.7444\n",
      "Epoch 12/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.2642 - acc: 0.9076 - val_loss: 0.9353 - val_acc: 0.7497\n",
      "Epoch 13/20\n",
      "15998/15998 [==============================] - 3s 196us/step - loss: 0.2424 - acc: 0.9204 - val_loss: 1.2600 - val_acc: 0.6809\n",
      "Epoch 14/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.2165 - acc: 0.9285 - val_loss: 0.9781 - val_acc: 0.7569\n",
      "Epoch 15/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.2004 - acc: 0.9377 - val_loss: 0.9664 - val_acc: 0.7647\n",
      "Epoch 16/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.1846 - acc: 0.9401 - val_loss: 1.0507 - val_acc: 0.7532\n",
      "Epoch 17/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.1685 - acc: 0.9464 - val_loss: 1.0910 - val_acc: 0.7497\n",
      "Epoch 18/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.1575 - acc: 0.9454 - val_loss: 1.1019 - val_acc: 0.7447\n",
      "Epoch 19/20\n",
      "15998/15998 [==============================] - 3s 196us/step - loss: 0.1589 - acc: 0.9494 - val_loss: 1.2029 - val_acc: 0.7397\n",
      "Epoch 20/20\n",
      "15998/15998 [==============================] - 3s 195us/step - loss: 0.1473 - acc: 0.9503 - val_loss: 1.1731 - val_acc: 0.7429\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/6\n",
      "15998/15998 [==============================] - 219s 14ms/step - loss: 2.7339 - acc: 0.1411 - val_loss: 2.4380 - val_acc: 0.1940\n",
      "Epoch 2/6\n",
      "15998/15998 [==============================] - 219s 14ms/step - loss: 2.2833 - acc: 0.2415 - val_loss: 2.2338 - val_acc: 0.2591\n",
      "Epoch 3/6\n",
      "15998/15998 [==============================] - 217s 14ms/step - loss: 1.9498 - acc: 0.3416 - val_loss: 1.8453 - val_acc: 0.3626\n",
      "Epoch 4/6\n",
      "15998/15998 [==============================] - 218s 14ms/step - loss: 1.6960 - acc: 0.4221 - val_loss: 1.5178 - val_acc: 0.4749\n",
      "Epoch 5/6\n",
      "15998/15998 [==============================] - 210s 13ms/step - loss: 1.4494 - acc: 0.5006 - val_loss: 1.3540 - val_acc: 0.5466\n",
      "Epoch 6/6\n",
      "15998/15998 [==============================] - 210s 13ms/step - loss: 1.2729 - acc: 0.5652 - val_loss: 1.2291 - val_acc: 0.5916\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = LSTM(128)(embedded_sequences)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "history = model.fit(x_train, y_train, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=6, \n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hausaufgaben\n",
    "\n",
    "1) Nachbauen\n",
    "2) Parameter-Suche: \n",
    "a) Optimierung von CNN\n",
    "b) Optimierung des LSTMs (2 Layer mit Dropout dazwischen, Bidirektional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
